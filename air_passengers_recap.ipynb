{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href=\"http://www.datascience-paris-saclay.fr\">Paris Saclay Center for Data Science</a>\n",
    "# <a href=https://ramp.r0h.eu/problems/air_passengers>RAMP</a> on predicting the number of air passengers\n",
    "\n",
    "<i> Balázs Kégl (LAL/CNRS), Alex Gramfort (LTCI/Telecom ParisTech), Djalel Benbouzid (UPMC), Mehdi Cherti (LAL/CNRS) </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: the goal this project was to build a regression model to predict the number of passengers per flight for a given company in the US. To to this, we (Sarah and I) decided to :\n",
    "- build an external data file with additional information of flights, weather conditions, locations, holidays... that could help the model\n",
    "- build an adapated regression model after many different tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This document is only containing examples of how we managed to find a solution to our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some of the commands we needed to build our dataframe\n",
    "\n",
    "%matplotlib inline\n",
    "import imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns; sns.set()\n",
    "import os\n",
    "import pandas_profiling\n",
    "import holidays\n",
    "import geopy\n",
    "import scipy\n",
    "import datetime\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from geopy.geocoders import Nominatim\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Visualizing the external data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide which features to add, we decided to brainstorm on information that could help our model, and tested each feature one by one. When a feature improved our model, we kept it. Otherwise, we deleted it.  \n",
    "This is our final external data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_data = pd.read_csv(\"submissions/starting_kit/external_data.csv\")\n",
    "external_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building a feature extractor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we built a feature extractor function, meant to link our original data frame with the one we had just built.  \n",
    "This is the final result :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected feature extractor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "import math \n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "def compute_distance(X_encoded):\n",
    "    return X_encoded.apply(\n",
    "        lambda x: geodesic(\n",
    "            (x[\"d_latitude_deg\"],x[\"d_longitude_deg\"]),\n",
    "            (x[\"a_latitude_deg\"],x[\"a_longitude_deg\"])\n",
    "        ).km,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "cols_to_norm = [\"WeeksToDeparture\", \"std_wtd\", \"d_Max TemperatureC\",\n",
    "                    \"d_MeanDew PointC\",\n",
    "                    \"d_Max Humidity\",\n",
    "                    \"d_Max Sea Level PressurehPa\",\n",
    "                    \"d_Max VisibilityKm\",\n",
    "                    \"d_Mean VisibilityKm\",\n",
    "                    \"d_Min VisibilitykM\",\n",
    "                    \"d_Max Wind SpeedKm/h\",\n",
    "                    \"d_Mean Wind SpeedKm/h\",\n",
    "                    \"d_Precipitationmm\",\n",
    "                    \"d_CloudCover\",\n",
    "                    \"d_WindDirDegrees\",\n",
    "                    \"d_latitude_deg\",\n",
    "                    \"d_longitude_deg\",\n",
    "                    \"d_elevation_ft\",\n",
    "                    \"d_2018\",\n",
    "                    \"d_2017\",\n",
    "                    \"d_2016\",\n",
    "                    \"d_2015\",\n",
    "                    \"d_2017GDPPerCapita\",\n",
    "                    \"d_OtherAirports\",\n",
    "                    \"d_arrival_avg_WeeksToDeparture\",\n",
    "                    \"d_arrival_avg_stdwtd\",\n",
    "                    \"d_arrival_avg_output\",\n",
    "                    \"d_departure_avg_WeeksToDeparture\",\n",
    "                    \"d_departure_avg_stdwtd\",\n",
    "                    \"d_departure_avg_output\",\n",
    "                    \"d_day\",\n",
    "                    \"d_PercentageOnTime\",\n",
    "                    \"a_Max TemperatureC\",\n",
    "                    \"a_MeanDew PointC\",\n",
    "                    \"a_Max Humidity\",\n",
    "                    \"a_Max Sea Level PressurehPa\",\n",
    "                    \"a_Max VisibilityKm\",\n",
    "                    \"a_Mean VisibilityKm\",\n",
    "                    \"a_Min VisibilitykM\",\n",
    "                    \"a_Max Wind SpeedKm/h\",\n",
    "                    \"a_Mean Wind SpeedKm/h\",\n",
    "                    \"a_Precipitationmm\",\n",
    "                    \"a_CloudCover\",\n",
    "                    \"a_WindDirDegrees\",\n",
    "                    \"a_latitude_deg\",\n",
    "                    \"a_longitude_deg\",\n",
    "                    \"a_elevation_ft\",\n",
    "                    \"a_2018\",\n",
    "                    \"a_2017\",\n",
    "                    \"a_2016\",\n",
    "                    \"a_2015\",\n",
    "                    \"a_2017GDPPerCapita\",\n",
    "                    \"a_OtherAirports\",\n",
    "                    \"a_arrival_avg_WeeksToDeparture\",\n",
    "                    \"a_arrival_avg_stdwtd\",\n",
    "                    \"a_arrival_avg_output\",\n",
    "                    \"a_departure_avg_WeeksToDeparture\",\n",
    "                    \"a_departure_avg_stdwtd\",\n",
    "                    \"a_departure_avg_output\",\n",
    "                    \"a_day\",\n",
    "                    \"a_PercentageOnTime\",\n",
    "                    \"d_ManyFlights\",\n",
    "                    \"a_ManyFlights\",\n",
    "                    \"departure_arrival_interaction\",\n",
    "                    \"Distance\",\n",
    "                    \"year\",\n",
    "                    \"month\",\n",
    "                    \"day\",\n",
    "                    \"weekday\",\n",
    "                    \"week\",\n",
    "                    \"n_days\"]\n",
    " \n",
    "\n",
    "\n",
    "class FeatureExtractor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_df, y_array):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        X_encoded = X_df\n",
    "        path = os.path.dirname(__file__)\n",
    "        \n",
    "        ## External data processing\n",
    "        external_data = pd.read_csv(os.path.join(path,'external_data.csv'))\n",
    "        external_data.loc[:,\"Date\"] = pd.to_datetime(external_data.loc[:,\"Date\"])\n",
    "        \n",
    "        # Building column names for conditions at departure and arrival \n",
    "        col_dep = ['d_' + name for name in list(external_data.columns)]\n",
    "        col_arr = [w.replace('d_', 'a_') for w in col_dep]\n",
    "        \n",
    "        # Fitting the names of the first 2 columns to match our original dataframe \n",
    "        col_dep = [w.replace('d_AirPort', 'Departure') for w in col_dep]\n",
    "        col_dep = [w.replace('d_Date', 'DateOfDeparture') for w in col_dep]\n",
    "        col_arr = [w.replace('a_AirPort', 'Arrival') for w in col_arr]\n",
    "        col_arr = [w.replace('a_Date', 'DateOfDeparture') for w in col_arr]\n",
    "        \n",
    "        # Building 2 dataframes from data_add to get the information for the departure and arrival airports of each flight\n",
    "        # Departure airport \n",
    "        external_dataDeparture = external_data.copy()\n",
    "        external_dataDeparture.columns = col_dep\n",
    "        # Arrival airport\n",
    "        external_dataArrival = external_data.copy()\n",
    "        external_dataArrival.columns = col_arr\n",
    "        \n",
    "        # Merging them with X_encoded \n",
    "        X_encoded = X_df.copy()\n",
    "        X_encoded.loc[:,'DateOfDeparture'] = pd.to_datetime(X_encoded.loc[:,'DateOfDeparture'])\n",
    "        X_encoded = pd.merge(X_encoded, external_dataDeparture, how='left',left_on=['DateOfDeparture', 'Departure'],\n",
    "                             right_on=['DateOfDeparture', 'Departure'],sort=False)\n",
    "        X_encoded = pd.merge(X_encoded, external_dataArrival, how='left',left_on=['DateOfDeparture', 'Arrival'],\n",
    "                             right_on=['DateOfDeparture', 'Arrival'],sort=False) \n",
    "        \n",
    "        ### Feature engineering\n",
    "        ## Creating columns to distinguish between the two main airports for flights and the rest\n",
    "        X_encoded['d_ManyFlights'] = 0  \n",
    "        X_encoded['a_ManyFlights'] = 0\n",
    "        X_encoded.loc[X_encoded.loc[:,'Departure'] == 'ORD', \"d_ManyFlights\"] = 1\n",
    "        X_encoded.loc[X_encoded.loc[:,'Arrival'] == 'ORD', \"a_ManyFlights\"] = 1\n",
    "        X_encoded.loc[X_encoded.loc[:,'Departure'] == 'ATL', \"d_ManyFlights\"] = 1\n",
    "        X_encoded.loc[X_encoded.loc[:,'Arrival'] == 'ATL', \"a_ManyFlights\"] = 1\n",
    "\n",
    "        # Getting the interaction of departure and arrival on the output\n",
    "        # We inputted the average output by departure/arrival airport in external data\n",
    "        X_encoded[\"airportTraffic_interaction\"] = X_encoded.loc[:,\"d_departure_avg_output\"]*X_encoded.loc[:,\"a_arrival_avg_output\"]\n",
    "        X_encoded[\"DistanceToHoliday_interaction\"] = X_encoded.loc[:,\"d_DistanceToClosestHoliday\"]*X_encoded.loc[:,\"a_DistanceToClosestHoliday\"]\n",
    "        X_encoded[\"GDPPerCapita_interaction\"] = X_encoded.loc[:,\"d_2017GDPPerCapita\"]*X_encoded.loc[:,\"a_2017GDPPerCapita\"]\n",
    "        X_encoded[\"Region_interaction\"] = X_encoded.loc[:,\"d_M\"]*X_encoded.loc[:,\"a_M\"] + X_encoded.loc[:,\"d_S\"]*X_encoded.loc[:,\"a_S\"] + X_encoded.loc[:,\"d_N\"]*X_encoded.loc[:,\"a_N\"] + X_encoded.loc[:,\"d_W\"]*X_encoded.loc[:,\"a_W\"]\n",
    "        #X_encoded[\"MaxTemperature_interaction\"] = X_encoded.loc[:,\"d_Max TemperatureC\"]*X_encoded.loc[:,\"a_Max TemperatureC\"]\n",
    "\n",
    "        # Distance\n",
    "        X_encoded[\"Distance\"] = compute_distance(X_encoded)\n",
    "\n",
    "        \n",
    "        ## Categorical encoding of departure and arrival airports\n",
    "        X_encoded = X_encoded.join(pd.get_dummies(X_encoded.loc[:,'Departure'], prefix='d'))\n",
    "        X_encoded = X_encoded.join(pd.get_dummies(X_encoded.loc[:,'Arrival'], prefix='a'))\n",
    "                                   \n",
    "        ## Categorical encoding of the dates \n",
    "        X_encoded['year'] = X_encoded.loc[:,'DateOfDeparture'].dt.year\n",
    "        X_encoded['month'] = X_encoded.loc[:,'DateOfDeparture'].dt.month\n",
    "        X_encoded['day'] = X_encoded.loc[:,'DateOfDeparture'].dt.day\n",
    "        X_encoded['weekday'] = X_encoded.loc[:,'DateOfDeparture'].dt.weekday\n",
    "        X_encoded['week'] = X_encoded.loc[:,'DateOfDeparture'].dt.week\n",
    "        X_encoded['n_days'] = X_encoded.loc[:,'DateOfDeparture'].apply(lambda date: \n",
    "                                                                         (date - pd.to_datetime(\"1970-01-01\")).days)\n",
    "        \n",
    "        X_encoded = X_encoded.join(pd.get_dummies(X_encoded['year'], prefix='y'))\n",
    "        X_encoded = X_encoded.join(pd.get_dummies(X_encoded['month'], prefix='m'))\n",
    "        X_encoded = X_encoded.join(pd.get_dummies(X_encoded['day'], prefix='d'))\n",
    "        X_encoded = X_encoded.join(pd.get_dummies(X_encoded['weekday'], prefix='wd'))\n",
    "        X_encoded = X_encoded.join(pd.get_dummies(X_encoded['week'], prefix='w'))\n",
    "    \n",
    "        # Finally getting rid of departure, arrival, and date columns now that we do not need them to merge\n",
    "        X_encoded = X_encoded.drop('Departure', axis=1)\n",
    "        X_encoded = X_encoded.drop('Arrival', axis=1)\n",
    "        X_encoded = X_encoded.drop('DateOfDeparture',axis = 1)\n",
    "\n",
    "        # Scaling the data for selected columns\n",
    "        #scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)\n",
    "        #X_encoded[cols_to_norm] = scaler.fit_transform(X_encoded[cols_to_norm])\n",
    "\n",
    "    \n",
    "        return X_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building the regression function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 We first tried using a multi regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class Regressor(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.reg1 = Ridge()\n",
    "        self.reg2 = Lasso()\n",
    "        self.reg3 = LinearRegression()\n",
    "        self.metareg = RandomForestRegressor()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.reg1.fit(X, y)\n",
    "        self.reg2.fit(X, y)\n",
    "        self.reg3.fit(X, y)\n",
    "        X_combined = np.vstack([self.reg1.predict(X), self.reg2.predict(X), self.reg3.predict(X)]).T\n",
    "        self.metareg.fit(X_combined, y)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred1 = self.reg1.predict(X)\n",
    "        pred2 = self.reg2.predict(X)\n",
    "        pred3 = self.reg3.predict(X)\n",
    "        X_combined = np.vstack([pred1, pred2, pred3]).T\n",
    "        return self.metareg.predict(X_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 We switched to a GradientBoosting model and optimized our parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic combined regressor \n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "class Regressor(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.reg = GradientBoostingRegressor(n_estimators=8000, learning_rate=0.1, max_features='sqrt', min_samples_leaf=20, max_depth=10, min_samples_split=10)\n",
    "      \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.reg.fit(X,y)\n",
    "     \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred1 = self.reg.predict(X)\n",
    "     \n",
    "        return pred1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need this because the global variable __file__ (the path of the current file)\n",
    "# does not exist if we are in a notebook\n",
    "__file__ = 'submissions/starting_kit/'\n",
    "\n",
    "# Getting our inputs\n",
    "problem = imp.load_source('', 'problem.py')\n",
    "X_df, y_array = problem.get_train_data()\n",
    "\n",
    "# Transforming our inputs\n",
    "fe = FeatureExtractor()\n",
    "fe.fit(X_df, y_array)\n",
    "X_array = fe.transform(X_df)\n",
    "\n",
    "# Fitting our model\n",
    "reg = Regressor()\n",
    "reg.fit(X_array, y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the importance of each column and the prediction \n",
    "X_columns = X_array.columns\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "ordering = np.argsort(reg.reg.feature_importances_)[::-1][:50]\n",
    "\n",
    "importances = reg.reg.feature_importances_[ordering]\n",
    "feature_names = X_columns[ordering]\n",
    "\n",
    "x = np.arange(len(feature_names))\n",
    "plt.bar(x, importances)\n",
    "plt.xticks(x + 0.5, feature_names, rotation=90, fontsize=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing our parameters \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gradient_params = {\n",
    "    'n_estimators':[250,275,300], \n",
    "    'max_depth':[4,5,7]}\n",
    "model_boost = {\n",
    "    'GradientBoost': GridSearchCV(\n",
    "        GradientBoostingRegressor(min_samples_leaf = 5),\n",
    "        param_grid = gradient_params, \n",
    "        scoring = \"neg_mean_squared_error\").fit(X_array, y_array).best_estimator_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ramp_test_submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
